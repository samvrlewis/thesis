\chapter{Discussion}

\section{Assesment of Completed framework}

The performance, strength and weaknesses of each of the components of the completed framework are examined in this section. Overall, the framework performed well however there were some minor usability issues.

\subsection{Data Curation}

The data curation function of the framework converts binary TDF data into structured HDF files. Data from any type of sensor can be read using the data curation part of the framework as long as the data is in the TDF format. 

Once the data is in the HDF format it can be accessed through a JSON RESTful interface from any external application. This part of the framework could be used for any purpose where TDF formatted sensor data is needed to be universally accessible. 

Overall, the data curation part of the framework was sucessful in providing a script that could create a HDF file format for any TDF formatted sensor data. 

\subsection{Annotation Interface}

The annotation interface allows a user without technical expertise to annotate any data that can be accessed over the RESTful interface. If there is audio data provided it allows the user to playback parts of the audio to assist in the manual annotation process.

The launcher allows the user to quickly choose the sensor data to annotate while not needing to know anything about the underlying RESTful system or the way in which the data is retrieved or stored or submitted. Annotations were sent over the RESTful system and then stored alongside the sensor data. This allowed easy retrieval of annotated data and would allow simple access to any third party applications needing access to the annotation metadata with the sensor data. 

Loading prediction files is another feature of the annotation interface. After developing a classifier and running it over a time period, a prediction file can be saved and then be plotted over the data. This is useful for qualitatively determining the accuracy of a classifier. 

One arguable weakness of the annotation interface is that the classification categories are hardcoded. It would be simple to add a mechanism for the user to type in the classification categories wanted but this could cause inconsistency in the category names. Presently only a single line of code needs to be modified to change the categories. 

Another weakness is the responsiveness of the interface. If a lot of data is being annotated then the interface is slower to respond to user input. Additionally, it can take some time to access a large amount of data from the annotation interface. The code for the interface is largely unoptimised so the responsiveness could be improved by some profiling of the code. The speed issues when downloading large amounts of data is largely because of the speed of the RESTful interface. At the time of writing this takes 10 minutes to download 2 minutes of data. However, the backend of this system will be updated in the future which will help with this. 

The annotation interface worked well overall. There were some issues with speed, responsiveness and the fact that categories were hardcoded in however these are minor or expected to be fixed in the future. 

\subsection{Classifier Design}

The process of developing a classifier from annotated data was made to be as streamlined and simple as possible. It is simple to access the annotated data over the RESTful interface to use in training a classifier. 

The modular design of the feature function systems allows feature sets to be swapped in and out easily. This allows the performance of the classifier with different features to be found quickly and easily which is of great benefit when prototyping classifiers. 

The pipeline design allows classifiers and scalers to be persisted as one structure which allows new data to be classified without needing extra overhead to extract the classifier or scaler from the training data. 

The classifier design system allows quick and simple design of a classifier using previously annotated data. It is simple to swap features in and out and easy to use any machine learning algorithm. 

\subsection{Classifier Verification}

The performance of the classifier was able to be verified by two different processes. 

The first process involved using part of the annotated data to test the classifier. This allowed a confusion matrix and an accuracy number to be found. By splitting the annotated data so that different variations were used for both training and testing, a confidence interval was able to be found for a classifier. 

The second process used a moving window to classify the complete data set. Results could then be seen visually in the interface. This was a less precise process than the first process but mirrored what would happen in the field more closely. This could be improved upon by finding a metric for this method to vertify the performance more exactly. 

Though it was hard to use the verification process to get the performance of a classifier that would run in the field, the verification process still allowed the performance different classifiers to be analysed. In this way, the effectiveness of using different models and features could be found. 

\section{Results for use with feed effeciency trial}

To test the effectiveness of the framework and to act as a proof of concept, the framework was used to create a chew classifier for the cattle feed efficiency problem described in section 2.1. A small dataset was annotated and a classifier was created using this annotated data. 

This section details this procedure and lists the results of doing so. 

\subsection{Data Curation}

Data was converted to HDF using the \texttt{sd2hdf.py} script. This took approximately one hour per one gigabyte of TDF formatted data on a 2012 series MacBook Air running a 1.7 GHz Intel i5. Once the HDF files were generated, they were placed on a CSIRO server and were accessible over a RESTful interface. 

\subsection{Data Annotation}

Across the data from the feed effiency experiments discussed in section \ref{feedef}, 1133 different events were annotated. Because there is more variability in non chew events than chew events (because a non chew event encompasses all other behaviour), more non chew events were recorded than chew events. 

Annotation was performed by observing the video files, finding times when the cattle were exhibiting a particular behaviour and then using the annotation interface to annotate that period of time. For annotating chew events, the audio was used to precisely locate the start and end time of the events. For consistency, annotations were attempted to be kept to approximately 350ms, however the exact length was not critical as it could be changed with post processing. 

For variability, events were chosen across a number of different times. This helped to ensure that the classifier would be robust. 

\subsection{Classifier Design}

The annotated data was used to train a SVM model. A number of different features were trialled, these are listed below with explanation. The sensors that were used for the classifier were the acceloremeter, magnetometer and audio. Trials were attempted with the pressure and temperature sensor but this sensor did not increase the accuracy of any classifier. 

\begin{itemize}
\item Standard deviation - the standard deviation across a window of sensor data
\item Mean - the mean value across a window of sensor data
\item Min - the minimum value across a window of sensor data
\item Max - the maximum value across a window of sensor data
\item Half window difference - the difference in mean between the first half of a window and the second half
\item Max FFT power - the frequency of maximum power in a FFT of a window of sensor data
\item Average successive difference - the average difference between successive samples
\end{itemize}

For consistency, the window of all events was set to 350ms as this was the average length of a chew event. Events were fit to this length by finding the mid point of an event and then setting the start point of the event 175 ms before it and the end point 175 ms after it. 

\subsection{Classifier Results}

On the complete data set the classifier performed well in initial testing. The features that had the greatest effect on the performance of the classifier were the simple statistical features, that is the standard deviation, mean, min and max. With testing on the annotated dataset this gave a confidence interval of 83 - 90\% accuracy. This accuracy was found by splitting the annotation set in different ways and testing with one part of the set and testing with the other, as described in the method section. 

However, the accuracy of the classified suffered when trialled on new sections of data using the moving window method of testing. Qualitatively, it seemed as though the classifier classified most periods of movement as a chew event and idle periods as non chew events. This most likely is because of the large amounts of variation in non chew events. To capture this variation better, more non chew events would need to be annotated.

To confirm this, a classifier was built and tested on a small amount of data. This data was a 2 minute period of data from 01:45 to 01:47 on the 4th of December 2013. All events in this time period were annotated and a new classifier was trained using this data. Testing using this classifier gave a confidence interval of 89 - 94\% accuracy. The rolling window method also had much better accuracy with this method with most chew events being classified correct through the entire event. Figure \ref{classifycorrect} shows this over a few successive events. 

\begin{figure}[ht!]
\begin{center}
\leavevmode
\includegraphics[width=0.9\textwidth]{images/annotation_interface_predictions.png}
\end{center}
\caption{Predictions using the classifier over a short time period}
\label{classifycorrect}
\end{figure}

\todo{crop this image a bit and explain what its showing}

This shows that the framework works well when enough data has been annotated. More annotations would be needed to have the same levels of accuracy across the whole data set because the large amounts of variation between individual events. As a proof of concept, the classifier designed using the short time period worked well. 


\section{Future Directions}

There are some suggested future directions for this work concerning the annotation interface and classifier design and verification process.

\subsection{Web Annotation Interface}
It would be valuable to create a web based version for the annotation interface. Presently, to use the annotation interface, dependencies such as Python, matplotlib and scikit need to be installed first. These packages can be hard to install for a non technical user. Moving to a web based platform would allow for a user on any platform to use annotation interface. 

A web based version would also allow for a streamlined update process as there would only be a single version of the software running centrally, rather than many versions running locally.

The backend framework to build a web annotation interface exists as part of the functions in the \texttt{datatools.py} code, so it would only be a matter of writting a web interface to interface with this backend framework. 

\subsection{Video in Annotation Interface}
Presently, a user wishing to annotate using video needs to view the video and sensor data separately. If the video was displayed in the interface and synchronised to the displayed sensor data this would simplify the annotation process. 

\subsection{Advanced features}
More work could be performed in the area of finding features that work well for classification using the time series sensor data. While features that give good performance will vary from application to application there could be more generic features that are included by default 





